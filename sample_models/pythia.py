# -*- coding: utf-8 -*-
"""pythia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ikiDDqplcPual7uxqWshIPgK2aUcNWRj
"""

!pip install -q datasets transformers peft accelerate trl

!pip install --upgrade fsspec==2024.12.0 gcsfs==2024.12.0

from google.colab import drive

# Replace with the actual path to your JSON file in Google Drive
gdrive_json_path = '/content/drive/MyDrive/Asha_llm_data/women_jobs_chatbot_data.json'

import json
from datasets import Dataset

try:
    with open(gdrive_json_path, 'r') as f:
        data = json.load(f)
    dataset = Dataset.from_list(data)
    print(f"‚úÖ Dataset loaded with {len(dataset)} samples from Google Drive.")
    print(f"Sample dataset entry:\n{dataset[0]}")
except FileNotFoundError:
    print(f"‚ùå Error: JSON file not found at '{gdrive_json_path}'. Please check the path.")
    exit()
except json.JSONDecodeError:
    print(f"‚ùå Error: Could not decode JSON from '{gdrive_json_path}'. Please ensure it's a valid JSON file.")
    exit()

from transformers import AutoTokenizer

model_name = "EleutherAI/pythia-70m"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

print("--- Inspecting Dataset ---")
print(dataset)
print(dataset[0])

def preprocess_function_single(example):
    instruction = example.get('instruction')
    output = example.get('output')
    if instruction and output:
        try:
            input_text = f"{instruction} {output}"
            model_inputs = tokenizer(
                input_text,
                truncation=True,
                max_length=512,
                padding="max_length"
            )
            # Mask out padding tokens in labels
            labels = model_inputs["input_ids"].copy()
            labels = [label if mask == 1 else -100 for label, mask in zip(labels, model_inputs["attention_mask"])]
            model_inputs["labels"] = labels
            return model_inputs
        except Exception as e:
            print(f"Error processing example: {example}\nError: {e}")
            return None
    else:
        print(f"Warning: Skipping example due to missing 'instruction' or 'output': {example}")
        return None

few_samples_dataset = dataset.select(range(5))

tokenized_few_samples = few_samples_dataset.map(preprocess_function_single, num_proc=1)
print(tokenized_few_samples[0])

tokenized_dataset = dataset.map(preprocess_function_single, num_proc=1)
print(tokenized_dataset[0])

from peft import prepare_model_for_kbit_training

# If using a quantized model (like bitsandbytes), prepare it
model = prepare_model_for_kbit_training(model)

from transformers import AutoModelForCausalLM
from peft import LoraConfig, get_peft_model
import torch

# Load the base model
model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')

# Configure LoRA
lora_config = LoraConfig(
    r=8, # Further reduced rank
    lora_alpha=16, # Reduced alpha
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Apply LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

from peft import prepare_model_for_kbit_training, get_peft_model

# Make model ready for LoRA + gradient updates
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)

# Double-check it has trainable parameters
model.print_trainable_parameters()

from transformers import TrainingArguments

output_dir = "./pythia-70m-women-jobs-lora"
per_device_train_batch_size = 4
gradient_accumulation_steps = 4
optim = "adamw_torch"  # ‚Üê FIXED HERE
save_steps = 100
logging_steps = 10
learning_rate = 5e-4
max_grad_norm = 0.3
max_steps = 1000
warmup_ratio = 0.1
lr_scheduler_type = "cosine"

training_arguments = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    fp16=True,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=True,
    lr_scheduler_type=lr_scheduler_type,
    push_to_hub=False,
    gradient_checkpointing=True,
)

# Print number of trainable parameters
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print(f"Trainable parameters: {trainable_params} / {total_params}")

from trl import SFTTrainer

model.train()

trainer = SFTTrainer(
    model=model,
    train_dataset=tokenized_dataset,
    peft_config=lora_config,
    args=training_arguments,
)

# Step 3: Start training
print("\nüöÄ Starting the training process...")
trainer.train()
print("‚úÖ Training finished!")

# Save the LoRA adapter
trainer.model.save_pretrained(output_dir)

# Save tokenizer files (needed for inference)
tokenizer.save_pretrained(output_dir)

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

# Load base model and tokenizer
base_model = AutoModelForCausalLM.from_pretrained("EleutherAI/pythia-70m", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("EleutherAI/pythia-70m")

# Load LoRA adapter
model = PeftModel.from_pretrained(base_model, "./pythia-70m-women-jobs-lora")
model.eval()

# Inference code
prompt = "Suggest jobs for Software Technology\n"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
    )

print("üß† Model Output:\n", tokenizer.decode(outputs[0], skip_special_tokens=True))