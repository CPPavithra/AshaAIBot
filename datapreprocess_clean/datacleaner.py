# -*- coding: utf-8 -*-
"""AshaAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15elqmhUmlS8XXzxY_JbNw_5jqG4kk3Iv
"""

!pip install requests pandas transformers datasets torch scikit-learn nltk

!pip install --upgrade fsspec==2025.3.0

!pip install --upgrade fsspec==2024.12.0 datasets

import os
print(os.listdir())

import zipfile

with zipfile.ZipFile("resume.zip", "r") as zip_ref:
    zip_ref.extractall("resume_data")

with zipfile.ZipFile("pwc.zip", "r") as zip_ref:
    zip_ref.extractall("pwc")

print(os.listdir("resume_data"))  #resume data
print(os.listdir("pwc"))  #pwc

import pandas as pd

resume_df = pd.read_csv("/content/resume_data/resume_data.csv")
print(resume_df.head())  #this is to check resume csv

from google.colab import files
files.upload()
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets list
!kaggle datasets download -d asaniczka/1-3m-linkedin-jobs-and-skills-2024 --unzip -p /content/

import pandas as pd

linkedinskills_df = pd.read_csv("/content/job_skills.csv")
print(linkedinskills_df.head())
linkedinsummary_df = pd.read_csv("/content/job_summary.csv")
print(linkedinsummary_df.head())
linkedinposting_df = pd.read_csv("/content/linkedin_job_postings.csv")
print(linkedinposting_df.head())

import os
import pandas as pd

pwc_excel_path = "/content/pwc/diversityinclusion.xlsx"

if os.path.exists(pwc_excel_path):
    pwc_df = pd.read_excel(pwc_excel_path, sheet_name=None)  # Reads all sheets

    if pwc_df:  # Ensure sheets exist
        print("PwC Diversity Sheets Available:", list(pwc_df.keys()))  # List sheet names

        first_sheet = list(pwc_df.keys())[0]  # Get the first sheet name

        if not pwc_df[first_sheet].empty:  # Ensure it's not empty
            print("\nFirst Sheet Preview:\n", pwc_df[first_sheet].head())
        else:
            print("First sheet is empty.")
    else:
        print("No sheets found in the Excel file.")
else:
    print(f"File not found: {pwc_excel_path}")

pwc_excel_path = "/content/pwc/diversityinclusion.xlsx"

if os.path.exists(pwc_excel_path):
    pwc_df = pd.read_excel(pwc_excel_path, sheet_name=None)  # Reads all sheets
    print("PwC Diversity Sheets Available:", pwc_df.keys())  # List sheet names
    first_sheet = list(pwc_df.keys())[0]
    print("\nFirst Sheet Preview:\n", pwc_df[first_sheet].head())
else:
    print(f"File not found: {pwc_excel_path}")

combined_pwc_df = pd.concat(pwc_df.values(), ignore_index=True)
print(combined_pwc_df.shape)  # Check total rows and columns

!pip install google-search-results

import requests
import pandas as pd
from serpapi import GoogleSearch

print("SerpAPI is working correctly!")

import requests
import pandas as pd
from serpapi import GoogleSearch

# SerpAPI Key (Replace with your actual key)
SERP_API_KEY = "4e416c8febcb1c2a68d57622025263c30949ba9fec57ac7a782933d2dfd17981"

def search_google_jobs(user_query, user_location):
    """
    Fetches job listings from Google Jobs using SerpAPI based on user input (job field & location).
    """
    params = {
        "engine": "google_jobs",
        "q": user_query,
        #"q": "Data Scientist",
        "hl": "en",
        "location": user_location,
        #"location": "India",
        "api_key": SERP_API_KEY
    }

    search = GoogleSearch(params)
    results = search.get_dict()

    jobs = []
    if "jobs_results" in results:
        for job in results["jobs_results"]:
            jobs.append({
                "title": job.get("title", "N/A"),
                "company": job.get("company_name", "Not provided"),
                "location": job.get("location", "Location not provided"),
                "description": job.get("description", "No description available"),
                "job_link": f"https://www.google.com/search?q={job.get('job_id', '')}"
            })

    return jobs

# Example: Simulating user input from chatbot
user_field = input("Enter your field of interest (e.g., Economist, AI Researcher): ")
user_location = input("Enter your preferred job location (e.g., India, London, Remote): ")


#user_field = "Data Scientist"  # Replace with input("Enter your field of interest: ")
#user_location = "India"
job_results = search_google_jobs(user_field, user_location)

# Display the results
google_jobs_df = pd.DataFrame(job_results)
print(google_jobs_df.head())

# Optional: Print job listings in a readable format
if job_results:
    print("\nüîç Job Recommendations:\n")
    for idx, job in enumerate(job_results[:5], start=1):  # Show top 5 jobs
        print(f"{idx}. {job['title']} at {job['company']} ({job['location']})")
        print(f"   üìå Description: {job['description'][:150]}...")  # Show first 150 chars of description
        print(f"   üîó Apply Here: {job['job_link']}\n")
else:
    print("üòû No jobs found. Try a different search!")

#NOW COMBINING ALL DATASETS-

# List all variables in your environment
import pandas as pd
defined_dfs = [var for var in globals() if isinstance(globals()[var], pd.DataFrame)]
print(defined_dfs)

dfs = {
    "Google Jobs": google_jobs_df,
    "Resume": resume_df,
    "LinkedIn Skills": linkedinskills_df,
    "LinkedIn Summary": linkedinsummary_df,
    "LinkedIn Posting": linkedinposting_df,
    "PwC Combined": combined_pwc_df,
}

for name, df in dfs.items():
    print(f"\nüìå **{name}** Columns:\n")
    print(df.columns.tolist())  # Print column names as a list
    print("\n" + "-"*80)  # Separator for clarity

!pip install pandas numpy scikit-learn tqdm

# Rename columns for standardization
resume_df = resume_df.rename(columns=lambda x: x.strip().lower().replace(" ", "_"))
resume_df = resume_df.drop_duplicates()
import ast
# Ensure 'skills' is a list by splitting comma-separated values
resume_df['skills'] = resume_df['skills'].apply(lambda x: x.split(',') if isinstance(x, str) else [])

# Convert to lowercase, remove extra spaces & special characters
resume_df['skills'] = resume_df['skills'].apply(lambda x: [skill.strip().lower() for skill in x])

# Join skills into a single string for easy matching with job listings
resume_df['skills'] = resume_df['skills'].apply(lambda x: ", ".join(x))

#standardizing experience and job titles
resume_df['positions'] = resume_df['positions'].str.lower().str.strip()
resume_df['locations'] = resume_df['locations'].str.lower().str.strip()
resume_df['start_dates'] = pd.to_datetime(resume_df['start_dates'], errors='coerce')
resume_df['end_dates'] = pd.to_datetime(resume_df['end_dates'], errors='coerce')


#GOOGLE JOBS CLEANING
google_jobs_df = google_jobs_df.rename(columns=lambda x: x.strip().lower().replace(" ", "_"))
google_jobs_df = google_jobs_df.drop_duplicates()

# Handle missing values
google_jobs_df.fillna("", inplace=True)

# Standardize company & location
google_jobs_df['company'] = google_jobs_df['company'].str.lower().str.strip()
google_jobs_df['location'] = google_jobs_df['location'].str.lower().str.strip()
import re

def extract_keywords(description):
    words = re.findall(r'\b[a-zA-Z]{3,}\b', description.lower())  # Extract words with 3+ letters
    return ", ".join(set(words))  # Remove duplicates

google_jobs_df['keywords'] = google_jobs_df['description'].apply(extract_keywords)

# Cleaning job_skills in LinkedIn Skills Dataset
linkedinskills_df['job_skills'] = linkedinskills_df['job_skills'].apply(
    lambda x: x.split(', ') if isinstance(x, str) else x  # Convert comma-separated string to list
)
linkedinskills_df['job_skills'] = linkedinskills_df['job_skills'].apply(
    lambda x: ", ".join(x) if isinstance(x, list) else ""  # Convert list back to string
)

# Cleaning job_level & job_title in LinkedIn Posting Dataset
linkedinposting_df['job_level'] = linkedinposting_df['job_level'].astype(str).str.lower().str.strip()
linkedinposting_df['job_title'] = linkedinposting_df['job_title'].astype(str).str.lower().str.strip()

import pandas as pd

# Convert dictionary to DataFrame (if necessary)
if isinstance(pwc_df, dict):
    pwc_df = pd.concat(pwc_df.values(), ignore_index=True)

# Ensure all column names are strings before renaming
pwc_df.columns = [str(col) for col in pwc_df.columns]

# Rename columns (standardize naming)
pwc_df = pwc_df.rename(columns=lambda x: str(x).strip().lower().replace(" ", "_"))

# Drop unnecessary columns (if they exist)
drop_cols = ['unnamed:_0', 'unnamed:_1', 'unnamed:_2', 'unnamed:_3', 'unnamed:_4']
pwc_df = pwc_df.drop(columns=[col for col in drop_cols if col in pwc_df.columns])

# Handle job levels (fill missing values)
if 'job_level_after_fy20_promotions' in pwc_df.columns and 'job_level_before_fy20_promotions' in pwc_df.columns:
    pwc_df['job_level'] = pwc_df['job_level_after_fy20_promotions'].fillna(pwc_df['job_level_before_fy20_promotions'])
else:
    pwc_df['job_level'] = None  # Handle missing columns safely

# Ensure job_level is lowercase and clean
if 'job_level' in pwc_df.columns:
    pwc_df['job_level'] = pwc_df['job_level'].astype(str).str.lower().str.strip()

# Convert promotion status to binary (1 = Yes, 0 = No)
if 'promotion_in_fy21?' in pwc_df.columns:
    pwc_df['promotion_probability'] = pwc_df['promotion_in_fy21?'].map({'Yes': 1, 'No': 0})
else:
    pwc_df['promotion_probability'] = None  # Handle missing column safely

# Fill missing performance ratings with 'Unknown'
if 'fy20_performance_rating' in pwc_df.columns:
    pwc_df['performance_rating'] = pwc_df['fy20_performance_rating'].fillna('Unknown')
else:
    pwc_df['performance_rating'] = 'Unknown'  # Default if column is missing

dfs = {
    "Google Jobs": google_jobs_df,
    "Resume": resume_df,
    "LinkedIn Skills": linkedinskills_df,
    "LinkedIn Summary": linkedinsummary_df,
    "LinkedIn Posting": linkedinposting_df,
    "PwC Combined": combined_pwc_df,
}

for name, df in dfs.items():
    print(f"\nüìå **{name}** Columns:\n")
    print(df.columns.tolist())  # Print column names as a list
    print("\n" + "-"*80)  # Separator for clarity

import pandas as pd
import re

# ‚úÖ Assume your cleaned data is stored in these DataFrames
resume_clean_df = resume_df  # Your cleaned resume data
google_jobs_clean_df = google_jobs_df  # Your cleaned job listings

# ‚úÖ Save the cleaned data
resume_clean_df.to_csv("cleaned_resume_data.csv", index=False)
google_jobs_clean_df.to_csv("cleaned_google_jobs_data.csv", index=False)
linkedinskills_df.to_csv("cleaned_linkedin_skills.csv", index=False)
linkedinposting_df.to_csv("cleaned_linkedin_postings.csv", index=False)

print("‚úÖ Cleaned data saved successfully!")



#CLEANING STARTS FROM HERE

import pandas as pd
import re
from sklearn.preprocessing import LabelEncoder

# ‚úÖ Load the cleaned datasets
resume_clean_df = pd.read_csv("cleaned_resume_data.csv")
google_jobs_clean_df = pd.read_csv("cleaned_google_jobs_data.csv")
linkedinskills_df = pd.read_csv("cleaned_linkedin_skills.csv")  # Ensure this file exists
linkedinposting_df = pd.read_csv("cleaned_linkedin_postings.csv")  # Ensure this file exists

# ‚úÖ Debug: Print Column Names
print("Resume Columns:", resume_clean_df.columns)
print("Google Jobs Columns:", google_jobs_clean_df.columns)
print("LinkedIn Skills Columns:", linkedinskills_df.columns)
print("LinkedIn Posting Columns:", linkedinposting_df.columns)

# ‚úÖ Convert Dates to Years of Experience
resume_clean_df["start_dates"] = pd.to_datetime(resume_clean_df["start_dates"], errors="coerce")
resume_clean_df["end_dates"] = pd.to_datetime(resume_clean_df["end_dates"], errors="coerce")

# Fill missing end dates with today's date (assuming current job)
resume_clean_df["end_dates"] = resume_clean_df["end_dates"].fillna(pd.to_datetime("today"))

# Calculate experience in years
resume_clean_df["years_of_experience"] = (resume_clean_df["end_dates"] - resume_clean_df["start_dates"]).dt.days / 365
resume_clean_df["years_of_experience"] = resume_clean_df["years_of_experience"].fillna(0)  # Fill NaN with 0

# ‚úÖ Encode Job Levels (Ensure column exists)
encoder = LabelEncoder()

if "job_level" in google_jobs_clean_df.columns:
    resume_clean_df["job_level_encoded"] = encoder.fit_transform(resume_clean_df["positions"])
    google_jobs_clean_df["job_level_encoded"] = encoder.transform(google_jobs_clean_df["job_level"])
else:
    print("‚ö†Ô∏è Warning: 'job_level' column not found in Google Jobs Data!")

if "job_level" in linkedinposting_df.columns:
    linkedinposting_df["job_level_encoded"] = encoder.fit_transform(linkedinposting_df["job_level"])
else:
    print("‚ö†Ô∏è Warning: 'job_level' column not found in LinkedIn Postings!")

# ‚úÖ Convert Skills into List Format (for Matching)
resume_clean_df["skills"] = resume_clean_df["skills"].apply(lambda x: x.split(", ") if isinstance(x, str) else [])

if "job_skills" in linkedinskills_df.columns:
    linkedinskills_df["job_skills"] = linkedinskills_df["job_skills"].apply(
        lambda x: x.split(", ") if isinstance(x, str) else []
    )
    linkedinskills_df["job_skills"] = linkedinskills_df["job_skills"].apply(
        lambda x: ", ".join(x) if isinstance(x, list) else ""
    )
else:
    print("‚ö†Ô∏è Warning: 'job_skills' column not found in LinkedIn Skills Data!")

if "job_title" in linkedinposting_df.columns:
    linkedinposting_df["job_title"] = linkedinposting_df["job_title"].astype(str).str.lower().str.strip()
else:
    print("‚ö†Ô∏è Warning: 'job_title' column not found in LinkedIn Postings!")

# ‚úÖ Save Processed Data
resume_clean_df.to_csv("job_matching_ready_resumes.csv", index=False)
google_jobs_clean_df.to_csv("job_matching_ready_jobs.csv", index=False)
linkedinskills_df.to_csv("job_matching_ready_linkedin_skills.csv", index=False)
linkedinposting_df.to_csv("job_matching_ready_linkedin_postings.csv", index=False)

print("‚úÖ Job Matching Data Ready!")

from google.colab import files

# Replace 'job_matching_ready_resumes.csv' with the actual filename you need
files.download("job_matching_ready_resumes.csv")
files.download("job_matching_ready_jobs.csv")
files.download("job_matching_ready_linkedin_skills.csv")
files.download("job_matching_ready_linkedin_postings.csv")

import pandas as pd
import re

combined_pwc_df.to_csv("cleaned_pwc_data.csv", index=False)

import nltk

# ‚úÖ Download the necessary NLTK resources
nltk.download("punkt")  # Download the punkt tokenizer
nltk.download("wordnet")  # Download the WordNet data for lemmatization
nltk.download("stopwords")  # Download the stopwords data

# Modify this path if needed

nltk.download('punkt')

import re
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# ‚úÖ Reset NLTK data path
nltk.data.path = ['/root/nltk_data']  # or try '/usr/nltk_data' if this path doesn't work

# ‚úÖ Download necessary resources
nltk.download('wordnet')  # Lemmatizer
nltk.download('stopwords')  # Stopwords

# ‚úÖ Initialize NLP tools
stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    """Clean and preprocess text for AI chatbot and LLM model."""
    if isinstance(text, str):
        # Convert to lowercase
        text = text.lower()

        # Remove numbers and punctuation
        text = re.sub(r'\d+', '', text)
        text = re.sub(r'[^\w\s]', '', text)

        # Remove extra spaces
        text = re.sub(r'\s+', ' ', text).strip()

        # Split text into words
        words = text.split()

        # Lemmatize words and remove stopwords
        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]

        return " ".join(words)
    return ""

# ‚úÖ Load the cleaned datasets
resume_df = pd.read_csv("cleaned_resume_data.csv")
google_jobs_df = pd.read_csv("cleaned_google_jobs_data.csv")
pwc_df = pd.read_csv("cleaned_pwc_data.csv")  # PwC data for LLM

# ‚úÖ Apply text cleaning on relevant fields
resume_df["career_objective"] = resume_df["career_objective"].apply(clean_text)
resume_df["skills"] = resume_df["skills"].apply(clean_text)
resume_df["positions"] = resume_df["positions"].apply(clean_text)

google_jobs_df["description"] = google_jobs_df["description"].apply(clean_text)
google_jobs_df["keywords"] = google_jobs_df["keywords"].apply(clean_text)

# ‚úÖ Clean PWC data
#pwc_df["content"] = pwc_df["content"].apply(clean_text)  # Cleaning PwC textual content

# ‚úÖ Save Cleaned Text Data for AI Model & LLM
resume_df.to_csv("chatbot_ready_resumes.csv", index=False)
google_jobs_df.to_csv("chatbot_ready_jobs.csv", index=False)
pwc_df.to_csv("llm_ready_pwc_data.csv", index=False)  # PwC data for LLM

print("‚úÖ AI Chatbot & LLM Data Ready!")

from google.colab import files
files.download('chatbot_ready_resumes.csv')
files.download('chatbot_ready_jobs.csv')
files.download('llm_ready_pwc_data.csv')

# Print the first 3 rows of pwc_df
print(pwc_df.head(3))